{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e4bb4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c42f038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cb6e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7815d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_positions = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57dbb194",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('clothing.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2453dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clothing_image(filename):\n",
    "    \"\"\"Load clothing image with transparency.\"\"\"\n",
    "    clothing = cv2.imread(filename, cv2.IMREAD_UNCHANGED)\n",
    "    if clothing is None:\n",
    "        raise FileNotFoundError(f\"Clothing image {filename} not found.\")\n",
    "    return clothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8528047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_clothing_size_and_position(landmarks, frame_shape, chest_scaling=1.0):\n",
    "    \"\"\"Calculate clothing size and position starting from the neck.\"\"\"\n",
    "    frame_width, frame_height = frame_shape[1], frame_shape[0]\n",
    "\n",
    "    # Extract landmark positions\n",
    "    left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "    right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
    "\n",
    "    # Calculate average neck position\n",
    "    neck_x = int((left_shoulder[0] + right_shoulder[0]) / 2 * frame_width)\n",
    "    neck_y = int((left_shoulder[1] + right_shoulder[1]) / 2 * frame_height)\n",
    "\n",
    "    # Calculate chest width with scaling\n",
    "    shoulder_width = int(np.sqrt(\n",
    "        (right_shoulder[0] - left_shoulder[0]) ** 2 +\n",
    "        (right_shoulder[1] - left_shoulder[1]) ** 2\n",
    "    ) *  frame_width)\n",
    "    chest_width = int(shoulder_width * chest_scaling)\n",
    "\n",
    "    # Proportional height of the clothing\n",
    "    torso_height = int(chest_width * 1.5)\n",
    "\n",
    "    return chest_width, torso_height, neck_x, neck_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e34e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_landmarks(current_landmarks, buffer_size=5):\n",
    "    \"\"\"Smooth landmarks over time using a moving average.\"\"\"\n",
    "    global previous_positions\n",
    "    previous_positions.append(current_landmarks)\n",
    "    if len(previous_positions) > buffer_size:\n",
    "        previous_positions.pop(0)\n",
    "    return np.mean(previous_positions, axis=0)\n",
    "\n",
    "def overlay_clothing(frame, clothing, landmarks):\n",
    "    \"\"\"Overlay clothing image on the frame starting at the neck.\"\"\"\n",
    "    chest_width, torso_height, neck_x, neck_y = calculate_clothing_size_and_position(landmarks, frame.shape)\n",
    "\n",
    "    # Resize clothing image\n",
    "    resized_clothing = cv2.resize(clothing, (chest_width, torso_height))\n",
    "\n",
    "    # Calculate top-left corner of placement\n",
    "    top_left_x = neck_x - (chest_width // 2)\n",
    "    top_left_y = neck_y\n",
    "\n",
    "    # Clip to ensure the overlay stays within frame bounds\n",
    "    top_left_x = max(0, top_left_x)\n",
    "    top_left_y = max(0, top_left_y)\n",
    "    bottom_right_x = min(frame.shape[1], top_left_x + chest_width)\n",
    "    bottom_right_y = min(frame.shape[0], top_left_y + torso_height)\n",
    "\n",
    "    # Adjust dimensions for clipping\n",
    "    clipped_width = bottom_right_x - top_left_x\n",
    "    clipped_height = bottom_right_y - top_left_y\n",
    "    resized_clothing = resized_clothing[:clipped_height, :clipped_width]\n",
    "\n",
    "    # Handle transparency for overlaying clothing\n",
    "    if resized_clothing.shape[2] == 4:  # Image has an alpha channel\n",
    "        clothing_rgb = resized_clothing[:, :, :3]\n",
    "        alpha_channel = resized_clothing[:, :, 3] / 255.0\n",
    "    else:\n",
    "        clothing_rgb = resized_clothing\n",
    "        alpha_channel = np.ones((clipped_height, clipped_width), dtype=np.float32)\n",
    "\n",
    "    # Blend clothing with the frame\n",
    "    for i in range(clipped_height):\n",
    "        for j in range(clipped_width):\n",
    "            if alpha_channel[i, j] > 0:  # Apply where clothing is not fully transparent\n",
    "                frame[top_left_y + i, top_left_x + j] = (\n",
    "                    alpha_channel[i, j] * clothing_rgb[i, j] +\n",
    "                    (1 - alpha_channel[i, j]) * frame[top_left_y + i, top_left_x + j]\n",
    "                ).astype(np.uint8)\n",
    "\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a252428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load clothing image\n",
    "    clothing = load_clothing_image('clothing.jpg')  # Update with your clothing image path\n",
    "\n",
    "    # Open video capture\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video capture.\")\n",
    "        return\n",
    "\n",
    "    # Initialize the pose detection model\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Convert the image to RGB\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(rgb_frame)\n",
    "\n",
    "            if results.pose_landmarks:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                landmarks_array = np.array([[lm.x, lm.y, lm.visibility] for lm in landmarks])\n",
    "                smoothed_landmarks = smooth_landmarks(landmarks_array)\n",
    "\n",
    "                # Overlay clothing with smoothed landmarks\n",
    "                frame = overlay_clothing(frame, clothing, smoothed_landmarks)\n",
    "\n",
    "                # Draw pose landmarks on the frame for reference\n",
    "                mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "            # Display the frame with clothing overlay\n",
    "            cv2.imshow('Virtual Try-On', frame)\n",
    "\n",
    "            # Exit loop if 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca0b688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
